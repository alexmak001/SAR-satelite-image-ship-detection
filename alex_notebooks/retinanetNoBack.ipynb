{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import albumentations\n",
    "import albumentations.pytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow this: https://pseudo-lab.github.io/Tutorial-Book-en/chapters/en/object-detection/Ch3-preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for dataset\n",
    "def generate_box(obj):\n",
    "    \n",
    "    xmin = float(obj.find('xmin').text)\n",
    "    ymin = float(obj.find('ymin').text)\n",
    "    xmax = float(obj.find('xmax').text)\n",
    "    ymax = float(obj.find('ymax').text)\n",
    "    \n",
    "    return [xmin, ymin, xmax, ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(obj):\n",
    "    # only have ships\n",
    "    return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target(file): \n",
    "    with open(file) as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, \"xml\")\n",
    "        objects = soup.find_all(\"object\")\n",
    "\n",
    "        num_objs = len(objects)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in objects:\n",
    "            boxes.append(generate_box(i))\n",
    "            labels.append(generate_label(i))\n",
    "\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32) \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64) \n",
    "        \n",
    "        target = {}\n",
    "\n",
    "        \n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        \n",
    "        return target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01_10_12.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(os.listdir(\"annotations_yolo/\")))[0][:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShipDataset:\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.files = list(sorted(os.listdir(\"annotations_yolo/\")))\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_image = self.files[idx][:-3] + 'jpg'\n",
    "        file_label = self.files[idx][:-3] + 'xml'\n",
    "\n",
    "        img_path = os.path.join(\"images/\", file_image)\n",
    "        label_path = os.path.join(\"annotations/\", file_label)\n",
    "        \n",
    "        #print(img_path)\n",
    "        # Read an image with OpenCV, gray scale\n",
    "        image = cv2.imread(img_path,0)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image = torch.tensor(image,dtype=torch.float32)\n",
    "        image = image/255.0\n",
    "        image = torch.unsqueeze(image, dim=0)\n",
    "        target = generate_target(label_path)\n",
    "\n",
    "        #start_t = time.time()\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            #total_time = (time.time() - start_t)\n",
    "            image = augmented['image']\n",
    "        \n",
    "            \n",
    "        return image, target #, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include transformations/albumnations for dataset\n",
    "# resize to 640, include mirror images, as well as gaussian noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumenation augmentation ideas\n",
    "# RandomBrightnessContrast\n",
    "# GaussNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare an augmentation pipeline\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1859"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ShipDataset(\n",
    "    path = 'images/'\n",
    ")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 459)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(dataset, [1400,459])\n",
    "len(train_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 800, 800])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 800, 800])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=4,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/home/alex/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained_backbone' is deprecated since 0.13 and will be removed in 0.15, please use 'weights_backbone' instead.\n",
      "  warnings.warn(\n",
      "/home/alex/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights_backbone' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights_backbone=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights_backbone=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "retina = torchvision.models.detection.retinanet_resnet50_fpn(num_classes = 1, weights=False, pretrained_backbone = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(729.1529, device='cuda:0', grad_fn=<AddBackward0>) time: 174.15322041511536\n",
      "tensor(449.8226, device='cuda:0', grad_fn=<AddBackward0>) time: 177.15115880966187\n",
      "tensor(380.9455, device='cuda:0', grad_fn=<AddBackward0>) time: 178.27958512306213\n",
      "tensor(342.6867, device='cuda:0', grad_fn=<AddBackward0>) time: 179.09506797790527\n",
      "tensor(321.0643, device='cuda:0', grad_fn=<AddBackward0>) time: 178.67305636405945\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "retina.to(device)\n",
    "    \n",
    "# parameters\n",
    "params = [p for p in retina.parameters() if p.requires_grad] # select parameters that require gradient calculation\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "len_dataloader = len(train_loader)\n",
    "\n",
    "# about 4 min per epoch on Colab GPU\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    retina.train()\n",
    "\n",
    "    i = 0    \n",
    "    epoch_loss = 0\n",
    "    for images, targets in train_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        \n",
    "        #print(targets)\n",
    "        \n",
    "\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        #print(images)\n",
    "        loss_dict = retina(images, targets) \n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values()) \n",
    "\n",
    "        i += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += losses \n",
    "    print(epoch_loss, f'time: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a Nan value for loss, possible causes:\n",
    "#   Gradients exploding, use gradient clipping\n",
    "#   Data is not normalized, reduce values of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(retina, val_set, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[495.0399,  17.6567, 526.0787,  46.3136],\n",
      "        [585.3160, 714.2757, 599.2332, 728.5322],\n",
      "        [590.5747, 714.9469, 604.8293, 729.7625],\n",
      "        ...,\n",
      "        [602.5636, 443.1787, 609.7017, 450.4985],\n",
      "        [640.9027, 434.4478, 648.6368, 442.0447],\n",
      "        [603.6652, 415.1117, 612.5547, 423.9015]], device='cuda:0'), 'scores': tensor([0.4465, 0.4411, 0.3999, 0.3985, 0.3944, 0.3799, 0.3465, 0.3045, 0.2831,\n",
      "        0.2739, 0.2738, 0.2709, 0.2619, 0.2520, 0.2362, 0.2349, 0.2327, 0.2300,\n",
      "        0.2136, 0.2117, 0.2072, 0.2062, 0.2041, 0.2038, 0.2011, 0.2009, 0.2001,\n",
      "        0.1989, 0.1988, 0.1940, 0.1920, 0.1868, 0.1867, 0.1836, 0.1803, 0.1799,\n",
      "        0.1769, 0.1768, 0.1746, 0.1744, 0.1733, 0.1720, 0.1714, 0.1706, 0.1671,\n",
      "        0.1654, 0.1643, 0.1622, 0.1602, 0.1582, 0.1574, 0.1557, 0.1548, 0.1544,\n",
      "        0.1541, 0.1540, 0.1527, 0.1452, 0.1430, 0.1428, 0.1421, 0.1421, 0.1413,\n",
      "        0.1411, 0.1398, 0.1378, 0.1370, 0.1364, 0.1347, 0.1347, 0.1339, 0.1323,\n",
      "        0.1311, 0.1291, 0.1280, 0.1277, 0.1265, 0.1262, 0.1261, 0.1255, 0.1253,\n",
      "        0.1252, 0.1252, 0.1241, 0.1232, 0.1222, 0.1219, 0.1204, 0.1196, 0.1184,\n",
      "        0.1182, 0.1175, 0.1171, 0.1166, 0.1159, 0.1150, 0.1150, 0.1141, 0.1126,\n",
      "        0.1121, 0.1117, 0.1100, 0.1096, 0.1078, 0.1075, 0.1074, 0.1070, 0.1068,\n",
      "        0.1065, 0.1064, 0.1062, 0.1059, 0.1056, 0.1054, 0.1054, 0.1049, 0.1045,\n",
      "        0.1042, 0.1026, 0.1025, 0.1023, 0.1019, 0.1011, 0.1008, 0.1004, 0.0988,\n",
      "        0.0987, 0.0981, 0.0978, 0.0976, 0.0974, 0.0970, 0.0970, 0.0970, 0.0963,\n",
      "        0.0963, 0.0957, 0.0956, 0.0951, 0.0948, 0.0946, 0.0943, 0.0937, 0.0936,\n",
      "        0.0934, 0.0932, 0.0928, 0.0928, 0.0923, 0.0919, 0.0917, 0.0914, 0.0910,\n",
      "        0.0908, 0.0907, 0.0902, 0.0898, 0.0896, 0.0896, 0.0896, 0.0895, 0.0884,\n",
      "        0.0884, 0.0883, 0.0883, 0.0878, 0.0877, 0.0871, 0.0867, 0.0867, 0.0866,\n",
      "        0.0863, 0.0863, 0.0859, 0.0851, 0.0846, 0.0843, 0.0843, 0.0841, 0.0838,\n",
      "        0.0838, 0.0832, 0.0832, 0.0830, 0.0817, 0.0809, 0.0808, 0.0808, 0.0806,\n",
      "        0.0805, 0.0802, 0.0801, 0.0801, 0.0800, 0.0800, 0.0798, 0.0796, 0.0796,\n",
      "        0.0792, 0.0792, 0.0791, 0.0790, 0.0787, 0.0783, 0.0777, 0.0771, 0.0769,\n",
      "        0.0767, 0.0767, 0.0766, 0.0764, 0.0761, 0.0755, 0.0754, 0.0752, 0.0750,\n",
      "        0.0750, 0.0748, 0.0743, 0.0741, 0.0741, 0.0741, 0.0740, 0.0738, 0.0736,\n",
      "        0.0736, 0.0736, 0.0735, 0.0733, 0.0731, 0.0730, 0.0724, 0.0722, 0.0720,\n",
      "        0.0719, 0.0717, 0.0714, 0.0712, 0.0707, 0.0703, 0.0702, 0.0701, 0.0700,\n",
      "        0.0700, 0.0698, 0.0695, 0.0693, 0.0690, 0.0686, 0.0684, 0.0684, 0.0684,\n",
      "        0.0684, 0.0683, 0.0682, 0.0682, 0.0681, 0.0681, 0.0681, 0.0680, 0.0679,\n",
      "        0.0679, 0.0678, 0.0674, 0.0672, 0.0669, 0.0668, 0.0666, 0.0663, 0.0662,\n",
      "        0.0660, 0.0660, 0.0659, 0.0658, 0.0658, 0.0655, 0.0655, 0.0654, 0.0653,\n",
      "        0.0653, 0.0652, 0.0651, 0.0650, 0.0650, 0.0647, 0.0645, 0.0645, 0.0644,\n",
      "        0.0642, 0.0641, 0.0640, 0.0639, 0.0639, 0.0639, 0.0636, 0.0635, 0.0634,\n",
      "        0.0632, 0.0632, 0.0632], device='cuda:0'), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')}]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/alex/school/DSC180A/SAR-satelite-image-ship-detection/alex_notebooks/retinanetNoBack.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/school/DSC180A/SAR-satelite-image-ship-detection/alex_notebooks/retinanetNoBack.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m predictions \u001b[39m=\u001b[39m retina(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/school/DSC180A/SAR-satelite-image-ship-detection/alex_notebooks/retinanetNoBack.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(predictions)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alex/school/DSC180A/SAR-satelite-image-ship-detection/alex_notebooks/retinanetNoBack.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m totalLoss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m predictions\u001b[39m.\u001b[39;49mvalues()) \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/school/DSC180A/SAR-satelite-image-ship-detection/alex_notebooks/retinanetNoBack.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m totalValLoss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m totalLoss\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "totalValLoss = 0\n",
    "# switch off autograd\n",
    "with torch.no_grad():\n",
    "\t\t# set the model in evaluation mode\n",
    "\tretina.eval()\n",
    "\t\t# loop over the validation set\n",
    "\tfor (images, labelNbox) in val_set:\n",
    "\t\t# send the input to the device\n",
    "\t\t(images, labels, bboxes) = (images.to(device),\n",
    "\t\t\tlabelNbox[\"labels\"].to(device), labelNbox[\"boxes\"].to(device))\n",
    "\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t#print(images.shape)\n",
    "\t\timages = images.reshape(1,800,800)\n",
    "\t\t# need to add extra dimension for batch\n",
    "\t\timages = torch.unsqueeze(images, dim=0)\n",
    "\t\t#print(images.shape)\n",
    "\t\tpredictions = retina(images)\n",
    "\t\tprint(predictions)\n",
    "\t\ttotalLoss = sum(loss for loss in predictions.values()) \n",
    "\t\ttotalValLoss += totalLoss\n",
    "\t\t\t# calculate the number of correct predictions\n",
    "\t\t#valCorrect += (predictions[1].argmax(1) == labels).type(\n",
    "\t\t\t\t#torch.float).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions[0][\"scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
